\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Handgeschriebene Ziffer 5}}{1}{figure.1.1}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Aufbau des neuronalen Netzwerks hinsichtlich der einzelnen Layer.}}{3}{figure.2.1}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Percetron mit den Eingaben $x_1, x_2, x_3$ und der Ausgabe $\mathtt {output}$.}}{4}{figure.2.2}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Unterschiedliche M\IeC {\"o}glichkeiten der Entscheidungsfindung.}}{5}{figure.2.3}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Sigmoid Funktion $\sigma (z)$.}}{5}{figure.2.4}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Percetron mit zwei Eingaben -2 und einem Bias von 3.}}{6}{figure.2.5}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Halbaddierer mit den Eingaben $x_1$ und $x_2$.}}{6}{figure.2.6}
\contentsline {figure}{\numberline {2.7}{\ignorespaces Halbaddierer-Aufbau mit Perceptrons.}}{6}{figure.2.7}
\contentsline {figure}{\numberline {2.8}{\ignorespaces Vereinfachter $\mathtt {NAND}$ Gatter Aufbau mit Perceptrons.}}{7}{figure.2.8}
\contentsline {figure}{\numberline {2.9}{\ignorespaces Modifizieren von Weights und Biases schaffen lernendes Netzwerk.}}{7}{figure.2.9}
\contentsline {figure}{\numberline {2.10}{\ignorespaces Sigmoid Funktion $\sigma (z)$.}}{8}{figure.2.10}
\addvspace {10\p@ }
