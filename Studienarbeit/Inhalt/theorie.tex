\chapter{Theorie}
Dieses Kapitel beschäftigt sich mit den Grundlagen für die Entwicklung und Implementierung eines neuronalen Netzwerks. Die Notationen für die mathematischen Ausdrücke orientieren sich an dem Skript zur Vorlesung \textit{Artifical Intelligence} von Prof. Dr. Karl Stroetmann \cite{stroetmann:2017}.

\section{Das Neuron}
\label{chap:sigmoid}
Ein grundlegender Bestandteil des menschlichen Gehirns ist das Neuron. Bereits ein kleiner Ausschnitt in der Größe eines Reiskorns enthält über 10000 Neuronen, wobei jedes Neuron durchschnittlich 6000 Verbindungen mit anderen Neuronen bildet \cite{buduma:2017}. Dieses biologische Netzwerk ermöglicht dem Menschen, die Welt um ihn herum zu erleben. Das Ziel in diesem Abschnitt ist es, diese natürliche Struktur zu nutzen, um maschinelle Lernmodelle zu entwickeln, die Probleme auf analoge Weise lösen. Hierbei ist es nicht notwendig zu wissen wie das biologische Neuron funktioniert, noch wie ein ein Netzwerk aus biologischen Neuronen arbeitet. Stattdessen wird eine mathematische Abstraktion eines Neurons formuliert, welches die Grundlage für unser neuronales Netzwerk bildet. \\
Ein Neuron mit $n$ Eingaben wird als Paar $\langle w,b \rangle$ definiert, wobei der Vektor $\mathbf{w} \in \mathbb{R}^m$ den Gewichtungsvektor und $b \in \mathbb{R}$ die Vorbelastung repräsentieren. Konzeptionell gesehen, ist das Neuron eine Funktion $p$, welche den Eingabevektor $\mathbf{x} \in \mathbb{R}^m$ auf das Intervall $[0,1]$ abbildet. Diese Funktion ist definiert als \\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lclll}
	p(\mathbf{x};\mathbf{w},b) := a(\mathbf{x}\cdot\mathbf{w}+b),
\end{array}
$
\\[0.2cm]
wobei $a$ als die sogenannte Aktivierungsfunktion bezeichnet wird (siehe Abb. \ref{fig:perceptron}).
\begin{figure}[hbt]
	\centering
	\includegraphics[scale=0.25]{Bilder/sigmoid_neuron}
	\caption{Neuron mit Eingabevektor $\mathbf{x}$, Gewichtungsvektor $\mathbf{w}$ und Ausgabe $y$. \cite{buduma:2017}} 
	\label{fig:perceptron} 
\end{figure}

\noindent
In dieser Arbeit wird die Sigmoid-Funktion für die Aktivierung eines Neurons verwendet. Die Sigmoid-Funktion $S:\mathbb{R} \rightarrow [0,1]$ ist definiert als \\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lclll}
	S(t) := \frac{1}{1+\exp(-t)}.
\end{array}
$
\\[0.2cm]
Fällt die Betrachtung auf die Definition der Sigmoid-Funktion, lassen sich auf Basis der folgenden Überlegungen \\[0.2cm]
\hspace*{1.3cm}
$\ds\lim\limits_{x\rightarrow
-\infty} \exp(-x) = \infty$, \quad 
$\ds\lim\limits_{x\rightarrow+\infty} \exp(-x) = 0$, \quad and \quad
$\ds\lim\limits_{x\rightarrow\infty} \frac{1}{x} = 0$, 
\\[0.2cm]
die folgenden Eigenschaften ableiten: \\[0.2cm]
\hspace*{1.3cm}
$\ds \lim_{t\rightarrow-\infty} S(t) = 0$ \quad and \quad
$\ds \lim_{t\rightarrow+\infty} S(t) = 1$.
\\[0.2cm]
Die Sigmoid-Funktion $S$ konvergiert somit bei der Grenzwertbetrachtung gegen 0 bzw. 1. Eine weitere Eigenschaft der Sigmoid-Funktion besteht in deren Symmetrie (siehe Abb. \ref{fig:sigmoid_plot}). 
\begin{figure}[hbt]
	\centering
	\includegraphics[scale=0.7]{Bilder/sigmoid_plot}
	\caption{Die Sigmoid-Funktion. \cite{nielson:2017}} 
	\label{fig:sigmoid_plot} 
\end{figure}

\noindent
Bei einer Verschiebung der Funktion um $\frac{1}{2}$, liegt eine zentral symmetrische Funktion vor. \\[0.2cm]
\hspace*{1.3cm}
$S(-t)-\frac{1}{2}=-\left(S(t)-\frac{1}{2}\right)$.
\\[0.2cm]
Die Addition von $\frac{1}{2}$ auf beiden Seiten der Gleichung liefert \\[0.2cm]
\hspace*{1.3cm}
$S(-t)=1-S(t)$.
\\[0.2cm]
Fällt die Betrachtung zurück auf auf die Funktion $p$ zur Beschreibung des Neurons, liefert die Indexnotation die folgende Schreibweise. Mit \\[0.2cm]
\hspace*{1.3cm}
$\mathbf{w}=\langle w_1,\cdots ,w_m\rangle^T$
\\[0.2cm]
für den Gewichtungsvektor und \\[0.2cm]
\hspace*{1.3cm}
$\mathbf{x}=\langle x_1,\cdots ,x_m\rangle^T$
\\[0.2cm]
für den Eingabevektor, ergibt sich \\[0.2cm]
\hspace*{1.3cm}
$\ds p(\mathbf{x}; \mathbf{w}, b) = S\left(\biggl(\sum\limits_{i=1}^m x_i \cdot w_i\biggr) + b\right)$.
\\[0.2cm]

\section{Neuronales Netzwerk}
Das in dieser Arbeit angewandte Netzwerk nennt sich \textit{feedforward neural network} und beschreibt ein Netzwerk aus Neuronen, deren Informationsfluss keine Schleifen durchläuft. Die Topologie des neuronalen Netzwerk ist gegeben durch eine Zahl $L \in \mathbb{N}$ und einer Liste $[m(1),\cdots ,m(L)]$ mit $L$ natürlichen Zahlen. Hierbei bezeichnet $L$ die Anzahl der Schichten im neuronalen Netzwerk und für $i \in \{2,\cdots ,L\}$ gibt der Wert von $m(i)$ die Anzahl der Neuronen der $l$-ten Schicht an. Die erste Schicht wird in diesem Modell als Eingabeschicht bezeichnet. Sie enthält im Vergleich zu anderen Schichten keine Neuronen sondern Eingabeknoten. Die letzte Schicht (mit Index $L$) wird als Ausgabeschicht bezeichnet, wohingegen alle restlichen Schichten als verborgene Schichten bezeichnet werden. Liegen dem Netzwerk mehr wie nur eine verborgene Schicht vor, so bezeichnet man dieses als \textit{deep neural network} (siehe Abb. \ref{fig:neural_network_extended}).
\begin{figure}[hbt]
	\centering
	\includegraphics[scale=0.6]{Bilder/neural_network_extended}
	\caption{Aufbau des neuronalen Netzwerks hinsichtlich der einzelnen Schichten. \cite{nielson:2017}} 
	\label{fig:neural_network_extended} 
\end{figure}

\noindent
Für die erste Schicht, die Eingabeschicht, ist die Eingabedimension definiert durch $m(1)$. Analog ist die Ausgabedimension durch $m(L)$ definiert. Jeder Knoten der $l$-ten Schicht ist zu jedem Knoten der $(l+1)$-ten Schicht über eine Gewichtung verbunden. Weiterhin ist die Gewichtung des $k$-ten Neuron der $(l-1)$-ten Schicht zu dem $j$-ten Neuron in der $l$-ten Schicht gegeben durch $w_{j,k}^{(l)}$. Alle Gewichtungen in Schicht $l$ sind über die Gewichtungsmatrix $W^{(l)}$ zusammengefasst. Die Matrix ist eine $m(l) \times m(l-1)$ Matrix mit $\ds W^{(l)} \in \mathbb{R}^{m(l) \times m(l-1)}$ und ist definiert als
\\[0.2cm]
\hspace*{1.3cm}
$\ds W^{(l)} := \bigl( w_{j,k}^{(l)} \bigr)$.
\\[0.2cm]
Das $j$-te Neuron in Schicht $l$ hat ebenfalls noch eine Vorbelastung $b_j^{(l)}$. Die Vorbelastungen der Schicht $l$ werden ebenfalls über den Vorbelastungsvektor $\mathbf{b}^{(l)}$ zusammengefasst mit
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{b}^{(l)} := \langle b_1^{(l)}, \cdots, b_{m(l)}^{(l)} \rangle^\top$.
\\[0.2cm]
Für die Aktivierungsfunktion $a_j^{(l)}$ des $j$-ten Neurons in Schicht $l$ ergibt sich hierbei die folgende rekursive Definition:
\begin{enumerate}
\item Für die erste Schicht ergibt sich
      \begin{equation}
        \label{eq:feedforward1}
       a^{(1)}_j := x_j.
      \end{equation}
      Dies bedeutet, dass der Eingabevektor $\mathbf{x}$ die Aktivierung der Eingangsknoten darstellt.
\item Für alle anderen Knoten ergibt sich
      \begin{equation}
         \label{eq:feedforward2}
         a_j^{(l)}(\mathbf{x}) := 
             S\left(\Biggl(\sum\limits_{k=1}^{m(l-1)} w_{j,k}^{(l)}\cdot a_k^{(l-1)}(\mathbf{x})\Biggr) + b_{j}^{(l)}\right) 
        \quad \mbox{$\forall l \in \{2, \cdots, L\}$}.
\end{equation}
\end{enumerate}
Der Aktivierungsvektor der $l$-ten Schicht ist somit definiert durch
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{a}^{(l)} := \langle a_1^{(l)}, \cdots, a_{m(l)}^{(l)} \rangle^\top$.
\\[0.2cm]
Des Weiteren ist die Ausgabe des neuronalen Netzwerks für eine Eingabe $\mathbf{x}$ über die Neuronen der Ausgabeschicht gegeben. Der Ausgabevektor $\mathbf{o}(\mathbf{x}) \in \mathbb{R}^{m(L)}$ ist definiert über
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{o}(\mathbf{x}) := \langle a^{(L)}_1(\mathbf{x}), \cdots, a^{(L)}_{m(L)}(\mathbf{x}) \rangle^\top = \mathbf{a}^{(L)}(\mathbf{x})$.
\\[0.2cm]
Mit den zuvor definierten Gleichungen \ref{eq:feedforward1} und \ref{eq:feedforward2} kann nun betrachtet werden, wie Informationen durch Netzwerk verbreitet werden (\textbf{feedforward}).
\begin{enumerate}
\item Zu Beginn ist der Eingabevektor $\mathbf{x}$ gegeben und gespeichert in der Eingabeschicht des neuronalen Netzwerks: 
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathbf{a}^{(1)}(\mathbf{x}) := \mathbf{x}$.
\item Die erste Schicht von Neuronen, welche die zweite Schicht mit Knoten darstellt, wird aktiviert und berechnet über den Aktivierungsvektor $\mathbf{a}^{(2)}$ nach der Formel
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathbf{a}^{(2)}(\mathbf{x}) := S\bigl(W^{(2)} \cdot \mathbf{a}^{(1)}(\mathbf{x}) + \mathbf{b}^{(2)}\bigr) = 
                                        S\bigl(W^{(2)} \cdot \mathbf{x} + \mathbf{b}^{(2)}\bigr)
      $.
\item Die zweite Schicht von Neuronen, welche die dritte Schicht mit Knoten darstellt, wird aktiviert und berechnet über den Aktivierungsvektor $\mathbf{a}^{(3)}$ nach der Formel
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathbf{a}^{(3)}(\mathbf{x}) := S\bigl(W^{(3)} \cdot \mathbf{a}^{(2)}(\mathbf{x}) + \mathbf{b}^{(3)}\bigr)
                          = S\Bigl(W^{(3)} \cdot S\bigl(W^{(2)} \cdot \mathbf{x} + \mathbf{b}^{(2)}\bigr) + \mathbf{b}^{(3)}\Bigr)
        $
\item Dies wird solange weitergeführt bis die Ausgabeschicht erreicht wird und die Ausgabe
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathbf{o}(\mathbf{x}) := \mathbf{a}^{(L)}(\mathbf{x})$
      \\[0.2cm]
      berechnet wurde. 
\end{enumerate}
In der folgenden Betrachtung wird angenommen, dass dem neuronalen Netzwerk $n$ Trainingsdaten mit
\\[0.2cm]
\hspace*{1.3cm}
$\langle \mathbf{x}^{(i)}, \mathbf{y}^{(i)} \rangle$ \quad for $i=1,\cdots,n$ 
\\[0.2cm]
vorliegen, sodass 
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{x}^{(i)} \in \mathbb{R}^{m(1)}$ und $\mathbf{y}^{(i)} \in \mathbb{R}^{m(L)}$.
\\[0.2cm]
Das Ziel ist eine Belegung der Gewichtungsmatrix $W^{(l)}$ und dem Vorbelastungsvektor $b^{(l)}$ zu finden, damit
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{o}\bigl(\mathbf{x}^{(i)}\bigr) = \mathbf{y}^{(i)}$ \quad for all $i \in \{1,\cdots,n\}$.
\\[0.2cm]
In der Regel wird es nicht möglich sein die Gleichungen für alle $i \in \{ 1, \cdots ,n \}$ zu erfüllen, weshalb es gilt den Fehler zu minimieren. An dieser Stelle fällt die Betrachtung auf die quadratische Fehlerkostenfunktion C, die definiert ist als
\\[0.2cm]
\hspace*{1.3cm}
$\ds C\Bigr(W^{(2)}, \cdots, W^{(L)}, \mathbf{b}^{(2)}, \cdots, \mathbf{b}^{(L)};
     \mathbf{x}^{(1)}, \mathbf{y}^{(1)}, \cdots, \mathbf{x}^{(n)},\mathbf{y}^{(n)} \Bigr) := 
 \frac{1}{2 \cdot n} \cdot \sum\limits_{i=1}^n \Bigl(\mathbf{o}\bigl(\mathbf{x}^{(i)}\bigr) - \mathbf{y}^{(i)}\Bigr)^2
$.
\\[0.2cm]
An dieser Stelle ist zu berücksichtigen, dass die Kostenfunktion hinsichtlich der Trainingsdaten $\langle \mathbf{x}^{(i)},\mathbf{y}^{(i)} \rangle$ additiv ist. Eine Vereinfachung der Notation kann vorgenommen werden, wenn die Betrachtung bei der Kostenfunktion auf ein Trainingsbeispiel $\langle \mathbf{x},\mathbf{y} \rangle$ fällt. Dazu wird
\\[0.2cm]
\hspace*{1.3cm}
$\ds C_{\mathbf{x}, \mathbf{y}}\Bigr(W^{(2)}, \cdots, W^{(L)}, \mathbf{b}^{(2)}, \cdots, \mathbf{b}^{(L)}\Bigr) := 
 \frac{1}{2} \cdot \Bigl(\mathbf{a}^{(L)}\bigl(\mathbf{x}\bigr) - \mathbf{y}\Bigr)^2
$
\\[0.2cm]
definiert. Die Definition der Kostenfunktion lautet:
\\[0.2cm]
$\ds C\Bigr(W^{(2)}, \cdots, W^{(L)}, \mathbf{b}^{(2)}, \cdots, \mathbf{b}^{(L)};
     \mathbf{x}^{(1)}, \mathbf{y}^{(1)}, \cdots, \mathbf{x}^{(n)},\mathbf{y}^{(n)} \Bigr) := 
 \frac{1}{n} \cdot \sum\limits_{i=1}^n C_{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}}\Bigr(W^{(2)}, \cdots W^{(L)}, \mathbf{b}^{(2)}, \cdots, \mathbf{b}^{(L)}\Bigr) 
$.
\\[0.2cm]
Für den weiteren Verlauf dieser Arbeit wird die Notation $C_{\mathbf{x}, \mathbf{y}}$ für den Ausdruck
\\[0.2cm]
\hspace*{1.3cm}
$\ds C_{\mathbf{x}, \mathbf{y}}\Bigr(W^{(2)}, \cdots, W^{(L)}, \mathbf{b}^{(2)}, \cdots, \mathbf{b}^{(L)}\Bigr)$
\\[0.2cm]
eingeführt.


\section{Backpropagation}
\label{chap:backprop}
Der Backpropagation-Algorithmus ist ein überwachtes Lernverfahren zur Bestimmung der Gewichtungen in einem mehrschichtigen feedforward Netzwerk. Ziel des Algorithmus ist die iterative Bestimmung der Gewichtungen derart, dass der ermittelte Ausgabewert des neuronalen Netzwerks mit dem tatsächlichen übereinstimmt. Der Backpropagation-Algorithmus lässt sich auch als numerisches Verfahren charakterisieren, welcher das Minimum der Fehlerfunktion eines bestimmten Lernproblems durch Abstieg in der Gradientenrichtung (Gradient Descent) sucht \cite[S. 71]{zander:2013}.
In dieser Arbeit wird der Algorithmus für die effiziente Berechnung der partiellen Ableitungen der Kostenfunktion $C$ nach der Gewichtung $w_{j,k}^{(l)}$ und der Vorbelastungen $b_j^{(l)}$ verwendet. Hierzu wird in dem folgenden Kapitel das Werkzeug für dessen Berechnung gegeben. \\

\noindent
Für die Backpropagation-Gleichungen werden zunächst diverse Hilfsgrößen definiert, welche die spätere Notation vereinfachen sollen. \\
Die Definition von $z_j^{(l)}$, soll die Eingabe der Aktivierungsfunktion $S$ des $j$-ten Neuron der $l$-ten Schicht darstellen:
\\[0.2cm]
\hspace*{1.3cm}
$\ds z_j^{(l)} := \left(\sum\limits_{k=1}^{m(l-1)}  w_{j,k}^{(l)} \cdot a_k^{(l-1)}\right) + b_j^{(l)}$
\quad $\forall j \in \{1, \cdots, m(l)\}$ und $\forall l \in \{2,\cdots,L\}$.
\\[0.2cm]
Im Wesentlichen ist $z_j^{(l)}$ die Eingabe der Sigmoid-Funktion, damit die Aktivierung $a_j^{(l)}$ mit
\\[0.2cm]
\hspace*{1.3cm}
$a_j^{(l)} = S(z_j^{(l)})$.
\\[0.2cm]
berechnet werden kann. Angenommen im $j$-ten Neuron der $l$-ten Schicht erfahren die Berechnungen eine zusätzliche Änderung $\Delta z_j^{(l)}$.  Dadurch erfährt die Aktivierung des nachgelagerten Neuron den folgenden Zusatz: 
\\[0.2cm]
\hspace*{1.3cm}
$a_j^{(l)}=S(z_j^{(l)}+\Delta z_j^{(l)})$.
\\[0.2cm]
Diese Änderungen wird durch das neuronale Netzwerk weitergeleitet und kann entsprechende Auswirkungen auf die korrekte Bestimmung der Eingaben haben. Damit Fehler vermieder werden empfiehlt sich im Hinblick auf die partielle Ableitung der Kostenfunktion $C_{\mathbf{x},\mathbf{y}}$ nach der Gewichtung $w_{j,k}^{(l)}$ und den Vorbelastungen $b_j^{(l)}$ zuerst die Berechnung der partiellen Ableitung von $C_{\mathbf{x},\mathbf{y}}$ nach $z_j^{(l)}$ durchzuführen. Hierzu sei folgende Definition gegeben:
\\[0.2cm]
\hspace*{1.3cm}
$\ds\varepsilon_j^{(l)} := \frac{\partial C_{\mathbf{x}, \mathbf{y}}}{\partial z_j^{(l)}}$ \quad $\forall j \in \{1, \cdots, m(l)\}$ und $\forall l \in \{2,\cdots, L\}$,
\\[0.2cm]
An dieser Stelle ist zu berücksichtigen, dass die partielle Ableitung $\ds\varepsilon_j^{(l)}$ abhängig von $\mathbf{x}$ und $\mathbf{y}$ ist. Weiterhin werden die Größen $\ds\varepsilon_j^{(l)}$ in dem Vektor
\\[0.2cm]
\hspace*{1.3cm}
$\boldsymbol{\varepsilon}^{(l)} := \left(
  \begin{array}[c]{c}
    \varepsilon_1^{(l)}      \\
    \vdots             \\
    \varepsilon_{m(l)}^{(l)}  
  \end{array}
  \right)
$.
\\[0.2cm]
zusammengefasst und als \textit{Fehler in Schicht l} bezeichnet.

\noindent
Mit den definierten Hilfsgrößen ist es nun möglich die vier Gleichung für die Backpropagation zu beschreiben. 
 
\begin{enumerate}
\item Die erste Gleichungen berechnet $\varepsilon^{(L)}_j$ für $l=L$, wobei $S'(x)$ die Ableitung der Sigmoid-Funktion bezeichnet. Mit dieser Gleichung wir der Fehler in der Ausgabeschicht berechnet.
\begin{equation}
	\label{eq:BP1}
	\varepsilon^{(L)}_j = (a_j^{(L)} - y_j) \cdot S'\bigl(z_j^{(L)}\bigr)
	\quad \mbox{$\forall j \in \{1, \cdots, m(L)\}$}. \tag{BP1}
\end{equation}
mit
\begin{equation}
	\label{eq:sigmoidPrime}
	S'(x) = (1-S(t)) \cdot S(t).
\end{equation}
Die Gleichung \ref{eq:BP1} kann auch als vektorisierte Schreibweise mittels dem Hardamard-Produkt \cite[S. 353]{walz:2016}, welche in dieser Arbeit mit dem Zeichen $\odot$ dargestellt ist, angegeben werden. Hierfür ergibt sich:
\begin{equation}
  \label{eq:BP1s}
\boldsymbol{\varepsilon}^{(L)} = (\mathbf{a}^{(L)} - \mathbf{y}) \odot S'\bigl(\mathbf{z}^{(L)}\bigr),  \tag{BP1v}
\end{equation}
wobei $S'(\mathbf{z^{(L)}})$ wie folgt definiert ist:
\\[0.2cm]
\hspace*{1.3cm}
$ S'\left(
  \begin{array}[c]{c}
   z_1^{(L)}      \\
   \vdots       \\
   z_{m(L)}^{(L)} 
  \end{array}
  \right) := \left(
  \begin{array}[c]{c}
   S'\bigl(z_1^{(L)}\bigr)      \\
   \vdots       \\
   S'\bigl(z_{m(L)}^{(L)}\bigr)
  \end{array}
  \right)
$.
\\[0.2cm]
    
\item Die zweite Gleichung berechnet $\varepsilon^{(L)}_j$ für alle $l<L$. Der Fehler $\boldsymbol{\varepsilon}^{(l+1)}$ von Schicht $l+1$ fließt hierbei in die Berechnung von $\boldsymbol{\varepsilon}^{(l)}$ ein.
\begin{equation}
	\label{eq:BP2}
	\varepsilon^{(l)}_j = \sum\limits_{i=1}^{m(l+1)} w_{i,j}^{(l+1)} \cdot 			\varepsilon^{(l+1)}_i \cdot S'\bigl(z^{(l)}_j\bigr) \quad \mbox{ $\forall j \in \{1, \cdots, m(l)\}$ und $\forall l \in \{2, \cdots, L-1\}$} \tag{BP2}
\end{equation}
Auch diese Gleichung kann in der vektorisierten Schreibweise angegeben werden:
\begin{equation}
  \label{eq:BP2v}
  \boldsymbol{\varepsilon}^{(l)} = \Bigl(\bigl(W^{(l+1)}\bigr)^\top \cdot \boldsymbol{\varepsilon}^{(l+1)}\Bigr) \odot
  S'\bigl(z^{(l)}\bigr) \quad \mbox{$\forall l \in \{2, \cdots, L-1\}$}. \tag{BP2v}
\end{equation}

\item Die nächste Gleichung ist durch die partielle Ableitung von $C_{\mathbf{x},\mathbf{y}}$ nach der Vorbelastung $b_j^{(l)}$ des $j$-ten Neuron in Schicht $l$ gegeben. Diese Gleichung gibt somit die Änderungsrate der Kosten hinsichtlich der Vorbelastungen an.
\begin{equation}
  \label{eq:BP3}
  \frac{\partial C_{\mathbf{x}, \mathbf{y}}}{b_j^{(l)}} = \varepsilon_j^{(l)}
  \quad \mbox{$\forall j \in \{1,\cdots,m(l)\}$ und $\forall l \in \{2, \cdots,l\}$} \tag{BP3}
\end{equation}
Die vektorisierte Schreibweise nimmt hierbei die folgende Form an:
\begin{equation}
  \label{eq:BP3v}
  \nabla_{\mathbf{b}^{(l)}} C_{\mathbf{x}, \mathbf{y}} = \boldsymbol{\varepsilon}^{(l)}
  \quad \mbox{$\forall l \in \{2, \cdots,l\}$} \tag{BP3v}
\end{equation}
Mit $\nabla_{\mathbf{b}^{(l)}}$ wird in dieser Arbeit der Gradient von $C_{\mathbf{x},\mathbf{y}}$ im Hinblick auf $b^{(l)}$ bezeichnet. 

\item Die letzte Gleichung ist durch die partielle Ableitung von $C_{\mathbf{x},\mathbf{y}}$ nach der Gewichtung $w_{j,k}^{(l)}$ gegeben. Diese Gleichung gibt somit die Änderungsrate der Kosten hinsichtlich der Gewichtungen an.
\begin{equation}
  \label{eq:BP4}
  \frac{\partial C_{\mathbf{x}, \mathbf{y}}}{\partial w_{j,k}^{(l)}} = a_k^{(l-1)} \cdot \varepsilon_j^{(l)}
  \quad \mbox{$\forall j \in \{1,\cdots,m(l)\}$, $\forall k \in \{1,\cdots,m(l-1)\}$, $\forall l \in \{2, \cdots,l\}$} \tag{BP4}
\end{equation}
Die vektorisierte Schreibweise liefert auch in diesem Fall die folgende Form:
\begin{equation}
  \label{eq:BP4v}
  \nabla_{W^{(l)}} C_{\mathbf{x}, \mathbf{y}} = \boldsymbol{\varepsilon}^{(l)} \cdot \bigl(\mathbf{a}^{(l-1)}\bigr)^\top
  \quad \mbox{$\forall l \in \{2, \cdots,l\}$} \tag{BP4v}
\end{equation}
Der Ausdruck $\boldsymbol{\varepsilon}^{(l)} \cdot \bigl(\mathbf{a}^{(l-1)}\bigr)^\top$ bezeichnet hierbei die Matrixmultiplikation zwischen dem Spaltenvektor $\boldsymbol{\varepsilon}^{(l)}$ als $m(l) \times 1$ Matrix und dem Reihenvektor $\bigl(\mathbf{a}^{(l-1)}\bigr)^\top$ als $1 \times m(l-1)$ Matrix.
\end{enumerate}

\noindent
Bei Anbetracht der Gleichungen \ref{eq:BP3} und \ref{eq:BP4v} wird nun ersichtlich, weshalb die Einführung des Vektors $\boldsymbol{\varepsilon}^{(l)}$ von Vorteil war. Dieser Vektor ermöglicht die partielle Ableitung nach den Gewichtungen sowie den Vorbelastungen für die Kostenfunktion. Ebenfalls wurde die vektorisierte Schreibweise für die einzelnen Gleichungen in diesem Abschnitt aus Effizienzgründen für die spätere Implementierung eingeführt. Dies liegt in der schnelleren Ausführung von Matrix-Vektor-Multiplikationen bei Interpretersprachen wie z.B. SetlX begründet. \\

\noindent
Der Backpropagation Algorithmus durchläuft die folgenden Schritte:
\begin{enumerate}
\item \textbf{Feedforward-Berechnung:}
\begin{enumerate}
\item Der Eingabevektor $\textbf{x}$ ist gegeben und gespeichert in der Eingabeschicht des neuronalen Netzwerks:
\\[0.2cm]
\hspace*{1.3cm}
$a^{(1)}(\mathbf{x}) := \mathbf{x}$.
\\[0.2cm]
\item Für jede weitere Schicht $l \in \{2,\cdots , L\}$ wird die Aktivierung 
\\[0.2cm]
\hspace*{1.3cm}
$a^{(l)}(\mathbf{x}) := S(W^{(l)} \cdot a^{(l-1)}(x) + \mathbf{b}^{(l)}$
\\[0.2cm] 
berechnet.
\end{enumerate}
\item \textbf{Backpropagation-Berechnung:}
\begin{enumerate}
\item Berechnung des Fehler in Schicht L (Ausgabeschicht) mit 
\\[0.2cm]
\hspace*{1.3cm}
$\boldsymbol{\varepsilon}^{(L)} = (\mathbf{a}^{(L)} - \mathbf{y}) \odot S'\bigl(\mathbf{z}^{(L)}\bigr)  $
\\[0.2cm] 

\item Berechnung der Fehler mittels \textit{backpropagate} für die Schichten $l = L-1, L-2, \cdots , 2$
\\[0.2cm]
\hspace*{1.3cm}
$\boldsymbol{\varepsilon}^{(l)} = \Bigl(\bigl(W^{(l+1)}\bigr)^\top \cdot \boldsymbol{\varepsilon}^{(l+1)}\Bigr) \odot
  S'\bigl(z^{(l)}\bigr)$
\\[0.2cm] 

\item Ausgabe der Gradienten der Kostenfunktion 
\\[0.2cm]
\hspace*{1.3cm}
$\nabla_{\mathbf{b}^{(l)}} C_{\mathbf{x}, \mathbf{y}} = \boldsymbol{\varepsilon}^{(l)}$ \quad und \quad
$\nabla_{W^{(l)}} C_{\mathbf{x}, \mathbf{y}} = \boldsymbol{\varepsilon}^{(l)} \cdot \bigl(\mathbf{a}^{(l-1)}\bigr)^\top$
\\[0.2cm] 
\end{enumerate}
\end{enumerate}

\noindent
Wie bereits beschrieben berechnet der Backpropagation-Algorithmus den Gradienten der Kostenfunktion für die einzelnen Trainingsbeispiele. In der Praxis ist es üblich den Backpropagation mit einem Lernalgorithmus zu kombinieren. Hierzu wird im folgenden Abschnitt auf das \textit{Gradient Descent Verfahren} eingegangen.

\section{Gradient Descent}
Gradient Descent, auch Verfahren des steilsten Abstiegs genannt, ist ein Verfahren mit dem allgemeine Optimierungsprobleme gelöst werden können. Die Kernidee des Algorithmus ist die iterative Optimierung von Parametern, um eine gegebene Kostenfunktion zu minimieren. \\

\noindent
Das Gradientenverfahren nähert sich einen Minimum einer Funktion 
\\[0.2cm]
\hspace*{1.3cm}
$f:\mathbb{R}^n \rightarrow \mathbb{R}$
\\[0.2cm] 
über eine Folge stetig kleiner werdende Funktionswerte. Dabei berechnet der Algorithmus den lokalen Gradienten der Fehlerfunktion und geht in Richtung des absteigenden Gradienten bis der Algorithmus gegen ein Minimum konvergiert (siehe Abb. \ref{fig:gradient_descent1}) \cite{aurelien:2017}. Hierbei ist
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{\widehat{x}} = \arg\min\limits_{\mathbf{x}\in \mathbb{R}^n} f(\mathbf{x})$
\\[0.2cm] 
der Wert von $\mathbf{x} \in \mathbb{R}^n$, der $f(x)$ minimiert. Formal ist folgendes gegeben:
\\[0.2cm]
\hspace*{1.3cm}
$\forall \mathbf{x} \in \mathbb{R}^n : f(\mathbf{x}) \geq f\Bigl(\arg\min\limits_{\mathbf{x}\in \mathbb{R}^n} f(\mathbf{x})\Bigr)$.
\\[0.2cm]
Die Funktion arg min ist nur dann definiert, wenn ein Minimum eindeutig ist. Ist die Funktion $f$ differenzierbar, ist die notwendige Bedingung für den Vektor $\mathbf{\widehat{x}} \in \mathbb{R}^n$ mit 
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{\widehat{x}} = \arg\min\limits_{\mathbf{x}\in \mathbb{R}^n} f(\mathbf{x}) \quad$ durch $\quad \nabla f(\mathbf{x}) = 0 \quad$ zu gewährleisten.
\\[0.2cm]
Der Gradient der Funktion $f$ ist dabei definiert als
\\[0.2cm]
\hspace*{1.3cm}
$\nabla f := \left(
 \begin{array}{c}
 \ds\frac{\partial f}{\partial\, x_1} \\
    \vdots                            \\[0.1cm]
 \ds\frac{\partial f}{\partial\, x_n} \\ 
 \end{array}
 \right).
$
\\[0.2cm]
Weiterhin ist für einen Startwert, gegeben durch den Vektor $\mathbf{x}_0 \in \mathbb{R}^n$,  eine Folge $(x_n)_{n \in \mathbb{N}}$ definiert, sodass 
\\[0.2cm]
\hspace*{1.3cm}
$f(x_{n+1}) \leq f(x_n) \quad \forall n \in \mathbb{N}$.
\\[0.2cm] 

\begin{figure}[hbt]
	\centering
	\includegraphics[scale=0.2]{Bilder/gradient_descent1}
	\caption{Verfahren des Gradient Descent. \cite{aurelien:2017}} 
	\label{fig:gradient_descent1} 
\end{figure}
\noindent
 

\noindent
Eine wichtige Größe im Gradient Descent Verfahren ist die Lernrate des Algorithmus. Ist die Lernrate zu klein gewählt, benötigt der Algorithmus viele Iterationen bis dieser gegen ein Minimum konvergiert. Dadurch erhöht sich auch dessen Laufzeit (siehe Abb. \ref{fig:gradient_descent2}).
\begin{figure}[hbt]
	\centering
	\includegraphics[scale=0.2]{Bilder/gradient_descent2}
	\caption{Kleine Lernrate beim Gradient Descent Verfahren. \cite{aurelien:2017}} 
	\label{fig:gradient_descent2} 
\end{figure}

\noindent
Ist auf der anderen Seite die Lernrate zu groß gewählt, kann das Minimum verpasst werden und der Abstand zum Minimum kann sich möglicherweise sogar vergrößern (siehe Abb. \ref{fig:gradient_descent3})
\begin{figure}[hbt]
	\centering
	\includegraphics[scale=0.2]{Bilder/gradient_descent3}
	\caption{Große Lernrate beim Gradient Descent Verfahren. \cite{aurelien:2017}} 
	\label{fig:gradient_descent3} 
\end{figure}

\noindent
Der Verlauf einer Kostenfunktion muss dabei nicht immer die Form einer Parabel annehmen, weshalb das Konvergieren gegen ein Minimum erschwert werden kann (siehe Abb. \ref{fig:gradient_descent4}). Zum einen könnte aufgrund der zufälligen Initialisierung der Algorithmus nur ein lokales Minimum finden, welches nicht so gut ist wie das globale Minimum (siehe linke Seite der Abbildung). Zum anderen kann es ein Weile Dauern bis das Minimum erreicht wird, wobei dieses nicht erreicht wird, falls der Algorithmus vorher abbrechen sollte (siehe rechte Seite der Abbildung).
\begin{figure}[hbt]
	\centering
	\includegraphics[scale=0.2]{Bilder/gradient_descent4}
	\caption{Schwierigkeiten beim Gradient Descent Verfahren. \cite{aurelien:2017}} 
	\label{fig:gradient_descent4} 
\end{figure}

\noindent
Die quadratische Fehlerkostenfunktion $C$ für das lineare Regressionsmodell weißt hingegen eine konvexe Form auf. Dies bedeutet, dass keine lokalen Minima sondern nur ein globales Minimum vorhanden ist. Des Weiteren liegt eine kontinuierliche Funktion vor, die keine plötzlichen Änderungen in ihrer Form erfährt. Somit kann mit dem Gradient Descent Verfahren garantiert werden, dass der Algorithmus gegen das globale Minimum konvergiert (siehe Abb. \ref{fig:gradient_descent5}).
\begin{figure}[hbt]
	\centering
	\includegraphics[scale=0.25]{Bilder/gradient_descent5}
	\caption{Anwendung des Gradient Descent Verfahren auf die quadratische Fehlerkostenfunktion. \cite{buduma:2017}} 
	\label{fig:gradient_descent5} 
\end{figure}

\section{Stochastic Gradient Descent}
In diesem Abschnitt fällt die Betrachtung auf das \textit{Stochastic Gradient Descent Verfahren}, welches auf dem \textit{Gradient Descent Verfahren} aufbaut und Anwendung in dieser Arbeit findet. Der Vorteil des Verfahrens liegt in der Approximation der quadratischen Fehlerkostenfunktion durch eine Teilmenge der zugrundeliegenden Daten. Fließen in die Berechnung der quadratischen Fehlerkostenfunktion eine große Menge von $n$ Datensätzen ein, kann dessen Berechnung kostenintensive Auswirkungen haben. \\

\noindent
Die im vorangegangenen Kapitel vorgestellten Gleichungen der Backpropagation beschreiben den Gradienten der Kostenfunktion für ein einzelnes Trainingsbeispiel $\langle\mathbf{x}, \mathbf{y}\rangle$. Besteht die Absicht das neuronales Netzwerk dieser Arbeit zu trainieren, müssen alle Trainingsdaten berücksichtigt werden. Für $n$ Trainingsdaten 
\\[0.2cm]
\hspace*{1.3cm}
$\langle\mathbf{x}^{(1)}, \mathbf{y}^{(1)}\rangle$,
$\langle\mathbf{x}^{(2)}, \mathbf{y}^{(2)}\rangle$,
$\cdots$,
$\langle\mathbf{x}^{(n)}, \mathbf{y}^{(n)}\rangle$,
\\[0.2cm]
wurde die quadratische Fehlerkostenfunktion bereits wie folgt definiert: 
\\[0.2cm]
\hspace*{1.3cm}
$\ds C\Bigr(W^{(2)}, \cdots, W^{(L)}, \mathbf{b}^{(2)}, \cdots, \mathbf{b}^{(L)};
     \mathbf{x}^{(1)}, \mathbf{y}^{(1)}, \cdots, \mathbf{x}^{(n)},\mathbf{y}^{(n)} \Bigr) := 
 \frac{1}{2 \cdot n} \cdot \sum\limits_{i=1}^n \Bigl(\mathbf{o}\bigl(\mathbf{x}^{(i)}\bigr) - \mathbf{y}^{(i)}\Bigr)^2
$.
\\[0.2cm]
Wenn die Berechnung der Gradienten für die quadratische Fehlerkostenfunktion hinsichtlich einer Gewichtungsmatrix $W^{(l)}$ oder einer Vorbelastung $b^{(l)}$ vorgenommen werden soll, müssen die Summen
\\[0.2cm]
\hspace*{1.3cm}
$\ds \frac{1}{2\cdot n} \cdot \sum\limits_{i=1}^n \frac{\partial C_{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}}}{\partial w_{j,k}^{(l)}}$
\quad and \quad
$\ds \frac{1}{2\cdot n} \cdot \sum\limits_{i=1}^n \frac{\partial C_{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}}}{\partial b_{j}^{(l)}}$
\\[0.2cm]
über alle Trainingsdaten für einen Iterationsschritt des \textit{Gradient Descent} berechnet werden. Dies kann mit einer großen Anzahl von Trainingdaten kostenintensive Auswirkungen haben, weshalb beim \textit{Stochastic Gradient Descent} für die Berechnung der Summen eine zufällige Teilmenge aus den Trainingsdaten für die Abschätzung herangezogen wird. Bei einer Teilmenge mit $m$ Trainingsdaten, liegt die folgende Abschätzung zugrunde:
\\[0.2cm]
$\ds \frac{1}{2\cdot n} \cdot \sum\limits_{i=1}^n \frac{\partial C_{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}}}{\partial w_{j,k}^{(l)}}
 \approx
 \frac{1}{2\cdot m} \cdot \sum\limits_{i=1}^m \frac{\partial C_{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}}}{\partial w_{j,k}^{(l)}}
$
\quad und \quad
$\ds \frac{1}{2\cdot n} \cdot \sum\limits_{i=1}^n \frac{\partial C_{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}}}{\partial b_{j}^{(l)}}
     \approx
     \frac{1}{2\cdot m} \cdot \sum\limits_{i=1}^m \frac{\partial C_{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}}}{\partial b_{j}^{(l)}}
$.
\\[0.2cm]
Die Namensgebung für diese Methode, ist durch die zufällige Trainingdatenauswahl zu erklären. Im Vergleich zur \textit{Gradient Descent} Methode, kann sie das Lernen in neuronalen Netzwerken erheblich beschleunigen, da nicht alle Trainingsdaten in den Prozess einfließen.