// s(izes): number of nodes in respective layers --> [3,2], two layer network with first layer 3 inputs & second layer 2 neurons
class network (s) {
	mNumLayers := #s;
	mSizes := s;
	mBiases := null;
	mWeights := null;

	// Random initialisation of weights and biases according to the network structure
	init := procedure() {
		computeRndBiases();
		computeRndWeights();
	};

	// Return output of network for 'a' as input
	// a is then list of calculated outputs
	feedforward := procedure(a) {
		for( i in {2..#mBiases} ) { 
			a := sigmoid( mWeights[i]*a + mBiases[i] );
		}
		return a;
	};

	// Stochastic gradient descent learning algorithm
	// training_data is list of tuples [x,y] representing the inputs x and the correct digit y
	// eta is learning rate
	sgd := procedure(training_data, epochs, mini_batch_size, eta, test_data) {
		if(test_data != null) {
			n_test := #test_data; 		
		}
		n := #training_data;
		
		for(j in {1..epochs}) {
			training_data := shuffle(training_data);
			// Get mini-batches from the training data to train the network
			mini_batches := [ training_data[k..k+mini_batch_size-1] : k in [1,mini_batch_size..n] ];
		
			for(mini_batch in mini_batches) {			
				update_mini_batch(mini_batch, eta);		
			} 
			
			// Visual output
			if(test_data != null) {
				ev := evaluate(test_data);
				print("Epoch $j$: $ev$ / $n_test$");
			}
			else {
				print("Epoch $j$ complete");
			}
		}
	};

	// Update of weights and biases by using backpropagation on single mini_batches
	// mini_batch is list of tuples [x,y] of the training data
	update_mini_batch := procedure(mini_batch, eta) {
		// Initialize gradients with 0, according to structure of weights and biases
		nabla_b := [ 0*mBiases[i] : i in {1..#mBiases}];
		nabla_w := [ 0*mWeights[i] : i in {1..#mWeights}];
		
		for([x,y] in mini_batch) {
			[delta_nabla_b, delta_nabla_w] := backprop(x,y);

			nabla_b := [ nabla_b[i] + delta_nabla_b[i] : i in {1..#nabla_b} ];
			nabla_w := [ nabla_w[i] + delta_nabla_w[i] : i in {1..#nabla_w} ];
		}
		
		// Adjust mWeights and mBiases
		this.mWeights := [ mWeights[i] - eta/#mini_batch * nabla_w[i] : i in {1..#mWeights} ];
		this.mBiases := [ mBiases[i] - eta/#mini_batch * nabla_b[i] : i in {1..#mBiases} ];
	};

	// Backpropagation to calculate the gradient for the cost function
	// x: training inputs
	// y: correct result for inputs x
	backprop := procedure(x,y) {this.s1 := now();
		// Initialize gradients with 0
		nabla_b := [ 0 : i in {1..#mBiases}];
		nabla_w := [ 0 : i in {1..#mWeights}];

		// Feedforward pass
		activation := x;
		// List of all activations (layer by layer)
		activations := [ la_matrix(x) ];
		activations += [0 : i in [1..#mBiases]];
		
		// List of z-vectors (layer by layer)
		zs := [0 : i in [1..#mBiases]];
	
		for(i in {2..#mBiases}) {
			zs[i] := mWeights[i] * activation + mBiases[i];
			
			activation := sigmoid(zs[i]);
			activations[i + 1] := la_matrix(activation);
		}

		// Backwards pass, output layer
		// la_hadamard() needs a matrix -> cdm is cost derivate in matrix form
		cdm := la_matrix( cost_derivative(activations[-1], y) );
		epsilon := la_hadamard( cdm, sigmoid_prime(zs[-1]));

		// No writing of nabla_b[-1] possible -> use length
		lb := #nabla_b;
		lw := #nabla_w;
		nabla_b[lb] := epsilon;	
		nabla_w[lw] := epsilon * activations[-2]!;			
		// Backwards pass, hidden layer(s)
		for( l in [mNumLayers-1, mNumLayers-2 .. 2] ) {
			sp := sigmoid_prime(zs[l]);
			epsilon := la_hadamard( mWeights[l+1]! * epsilon, sp );
			nabla_b[l] := epsilon;
			nabla_w[l] := epsilon * activations[l-1]!;
		}
		return [nabla_b, nabla_w];
	};

	// Returns sum of correct guesses after feedforwarding
	evaluate := procedure(test_data) {
		test_results := [[argmax(feedforward(x)) - 1, y]: [x, y] in test_data];	
		// Return sum of correct guesses
		return #[1 : [x,y] in test_results | x == y];
	};

	// Return vector of partial derivates
	cost_derivative := procedure(output_activations, y) {
		return (output_activations - y);
	};

	// Sigmoid function for vectors
	// 1.0/(1.0+exp(-z))
	sigmoid := procedure(z) {
		// z is a vector, so the function has to be used on every part of it
		return la_vector([ 1.0/(1.0 + exp(- z[i] )) : i in [1..#z] ]);
	};

	// Derivative of the sigmoid function, when z is a vector
	// sigmoid(z)*(1-sigmoid(z))
	sigmoid_prime := procedure(z) {
		s := sigmoid(z); 
		return la_matrix([ [ s[i] * (1 - s[i]) ] : i in [1..#s] ]);
	};

	// Compute random biases and weights in a shape fitting the declared network structure
	// mBiases: 	[ << <<b_layer1_neu1>> <<b_layer1_neu2>> ... >>, << <<b_layer2_neu1>> ... >>, ...] 
	// 				--> mBiases[layer][neuron][bias]	(bias is always one, because there is only one bias per neuron)
	computeRndBiases := procedure() {
		// i is each layer of the network (except the input layer)
		this.mBiases := [0] + [ computeRndMatrix(1, mSizes[i]) : i in [2..mNumLayers] ];
	};
	// mWeights: 	[ << <<w1_layer2_neu1 w1_layer2_neu2 ... >> << w2_layer2_neu1 ... >> ... >>, << <<w1_layer3_neu1 w1_layer3_neu2 ... >> << w2_layer3_neu1 ... >> ... >>, ...]
	//				--> mWeights[layer-1][neuron][weight for input neuron]
	computeRndWeights := procedure() {
		this.mWeights := [0] + [ computeRndMatrix(mSizes[i], mSizes[i+1]) : i in [1..mNumLayers-1] ];
	};

	// Computes matrix with random values based on a shape s
	// e.g. 	1, 2 --> << <<x>> <<y>> >>
	//			2, 1 --> << <<x y>> >>
	computeRndMatrix := procedure(row, col) {
		return la_matrix([
			[ ((random()-0.5)*2)/28 : p in [1..row] ] : q in [1..col]
		]);
	};

	// Returns index of max. value of x
	argmax := procedure(x) {
		[maxValue, maxIndex] := [x[1], 1];
		for (i in [2 .. #x] | x[i] > maxValue) {
			[maxValue, maxIndex] := [x[i], i];
		}
		return maxIndex;
	};
}