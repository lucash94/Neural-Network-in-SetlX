\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{stroetmann:2017}
\citation{buduma:2017}
\citation{buduma:2017}
\citation{nielson:2017}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Theorie}{3}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Das Neuron}{3}{section.2.1}}
\newlabel{chap:sigmoid}{{2.1}{3}{Das Neuron}{section.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Neuron mit Eingabevektor $\mathbf  {x}$, Gewichtungsvektor $\mathbf  {w}$ und Ausgabe $y$. \cite  {buduma:2017}}}{3}{figure.2.1}}
\newlabel{fig:perceptron}{{2.1}{3}{Neuron mit Eingabevektor $\mathbf {x}$, Gewichtungsvektor $\mathbf {w}$ und Ausgabe $y$. \cite {buduma:2017}}{figure.2.1}{}}
\citation{nielson:2017}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Die Sigmoid-Funktion. \cite  {nielson:2017}}}{4}{figure.2.2}}
\newlabel{fig:sigmoid_plot}{{2.2}{4}{Die Sigmoid-Funktion. \cite {nielson:2017}}{figure.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Neuronales Netzwerk}{4}{section.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Aufbau des neuronalen Netzwerks hinsichtlich der einzelnen Schichten. \cite  {nielson:2017}}}{5}{figure.2.3}}
\newlabel{fig:neural_network_extended}{{2.3}{5}{Aufbau des neuronalen Netzwerks hinsichtlich der einzelnen Schichten. \cite {nielson:2017}}{figure.2.3}{}}
\newlabel{eq:feedforward1}{{2.1}{5}{Neuronales Netzwerk}{equation.2.2.1}{}}
\newlabel{eq:feedforward2}{{2.2}{5}{Neuronales Netzwerk}{equation.2.2.2}{}}
\citation{zander:2013}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Backpropagation}{7}{section.2.3}}
\newlabel{chap:backprop}{{2.3}{7}{Backpropagation}{section.2.3}{}}
\newlabel{eq:BP1}{{{BP1}}{7}{Backpropagation}{AMS.4}{}}
\citation{walz:2016}
\newlabel{eq:sigmoidPrime}{{2.3}{8}{Backpropagation}{equation.2.3.3}{}}
\newlabel{eq:BP1s}{{{BP1v}}{8}{Backpropagation}{AMS.5}{}}
\newlabel{eq:BP2}{{{BP2}}{8}{Backpropagation}{AMS.6}{}}
\newlabel{eq:BP2v}{{{BP2v}}{8}{Backpropagation}{AMS.7}{}}
\newlabel{eq:BP3}{{{BP3}}{8}{Backpropagation}{AMS.8}{}}
\newlabel{eq:BP3v}{{{BP3v}}{8}{Backpropagation}{AMS.9}{}}
\newlabel{eq:BP4}{{{BP4}}{8}{Backpropagation}{AMS.10}{}}
\newlabel{eq:BP4v}{{{BP4v}}{8}{Backpropagation}{AMS.11}{}}
\citation{aurelien:2017}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Gradient Descent}{9}{section.2.4}}
\citation{aurelien:2017}
\citation{aurelien:2017}
\citation{aurelien:2017}
\citation{aurelien:2017}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Verfahren des Gradient Descent. \cite  {aurelien:2017}}}{10}{figure.2.4}}
\newlabel{fig:gradient_descent1}{{2.4}{10}{Verfahren des Gradient Descent. \cite {aurelien:2017}}{figure.2.4}{}}
\citation{buduma:2017}
\citation{buduma:2017}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Kleine Lernrate beim Gradient Descent Verfahren. \cite  {aurelien:2017}}}{11}{figure.2.5}}
\newlabel{fig:gradient_descent2}{{2.5}{11}{Kleine Lernrate beim Gradient Descent Verfahren. \cite {aurelien:2017}}{figure.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Gro\IeC {\ss }e Lernrate beim Gradient Descent Verfahren. \cite  {aurelien:2017}}}{11}{figure.2.6}}
\newlabel{fig:gradient_descent3}{{2.6}{11}{Gro√üe Lernrate beim Gradient Descent Verfahren. \cite {aurelien:2017}}{figure.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Stochastic Gradient Descent}{11}{section.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Schwierigkeiten beim Gradient Descent Verfahren. \cite  {aurelien:2017}}}{12}{figure.2.7}}
\newlabel{fig:gradient_descent4}{{2.7}{12}{Schwierigkeiten beim Gradient Descent Verfahren. \cite {aurelien:2017}}{figure.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Anwendung des Gradient Descent Verfahren auf die quadratische Fehlerkostenfunktion. \cite  {buduma:2017}}}{12}{figure.2.8}}
\newlabel{fig:gradient_descent5}{{2.8}{12}{Anwendung des Gradient Descent Verfahren auf die quadratische Fehlerkostenfunktion. \cite {buduma:2017}}{figure.2.8}{}}
\@setckpt{Inhalt/theorie}{
\setcounter{page}{14}
\setcounter{equation}{3}
\setcounter{enumi}{2}
\setcounter{enumii}{3}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{8}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{FancyVerbLine}{0}
\setcounter{Item}{22}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{12}
\setcounter{btxromaniannumeral}{0}
\setcounter{Definition}{0}
\setcounter{aufgabe}{0}
\setcounter{section@level}{1}
}
