// s(izes): number of neurons in respective layers --> [3,2], two layer network with first layer 3 neurons & second layer 2 neurons
class network (s) {
	num_layers := #s;
	sizes := s;
	biases := null;
	weights := null;

	init := procedure() {
		computeRndBiases();
		computeRndWeights();
	};

	// Return output of network for "a" as input
	feedforward := procedure(a) {
		a := la_matrix([a]);
		a := a!;
		
		for( [b,w] in zip(biases, weights) ) { 
			a := sigmoid_matrix( (w*a) + b );
		}
		return a;
	};

	sgd := procedure(training_data, epochs, mini_batch_size, eta, test_data) {
		if(test_data != null) {
			n_test := #test_data; 		
		}
		n := #training_data;
		
		for(j in {0..epochs}) {
			training_data := shuffle(training_data);
			// Get mini-bacthes from the training data to train the network
			mini_batches := [ training_data[k..k+mini_batch_size-1] : k in [1,mini_batch_size+1..n+1] ];
			
			c := 1;
			for(mini_batch in mini_batches) {
				update_mini_batch(mini_batch, eta);
				// ToDo: NaN without the last break
				c:=c+1;
				if(c==#mini_batches){break;}
			} 
			
			// Visual output
			if(test_data != null) {
				ev := evaluate(test_data);
				print("Epoch $j$: $ev$ / $n_test$");
			}
			else {
				print("Epoch $j$ complete");
			}
		}
	};

	update_mini_batch := procedure(mini_batch, eta) {
		[nabla_b, nabla_w] := getNabla_b_and_w();
		
		for([x,y] in mini_batch) {
			[delta_nabla_b, delta_nabla_w] := backprop(x,y);
			
			nabla_b := [ nb+dnb : [nb,dnb] in zip(nabla_b, delta_nabla_b) ];
			nabla_w := [ nw+dnw : [nw,dnw] in zip(nabla_w, delta_nabla_w) ];
		}
		
		// Adjust weights and biases
		this.weights := [ (w-(eta/#mini_batch)*nw) : [w,nw] in zip(this.weights, nabla_w) ];
		this.biases := [ (b-(eta/#mini_batch)*nb) : [b,nb] in zip(this.biases, nabla_b) ];
	};
	
	/* Computes the Cartesian product of two matrices or vectors */
	zip := procedure(l1, l2) {
		return toList(l1) >< toList(l2);
	};
	/* casts vector to list */
	toList := procedure(v) {
		return [v[i] : i in [1..#v]];
	};

	backprop := procedure(x,y) {
		[nabla_b, nabla_w] := getNabla_b_and_w();
		
		// Feedforward
		activation := x;
		// List of all activations (layer by layer)
		activations := [ la_matrix([x]) ];
		// List of z-vectors (layer by layer)
		zs := [];
		
		for([b,w] in zip(this.biases, this.weights)) {
			dot := w * activation;
			z := dot + b;
			zs += [z];
			
			activation := sigmoid_vector(z);
			activations += [ la_matrix( [ [activation[i]] : i in [1..#activation] ] ) ];
		}
		
		// backwards pass
		delta := hadamard_product( cost_derivative(activations[-1], y), sigmoid_prime(zs[-1]));
		
		// No writing of nabla_b[-1] possible -> use length
		lb := #nabla_b;
		lw := #nabla_w;
		
		nabla_b[lb] := delta;
		nabla_w[lw] := delta * activations[-2]!;	// ! = transpose
		
		for( l in {2..num_layers-1} ) {
			z := zs[-l];
			sp := sigmoid_prime(z);	
			delta := hadamard_product( (weights[-l+1]! * delta), sp );
			nabla_b[lb-l+1] := delta;
			nabla_w[lw-l+1] := delta * activations[-l-1];
		}
		return [nabla_b, nabla_w];
	};
	
	hadamard_product := procedure(a,b) {
		n := #a;
		m := #a[1];
		return la_matrix( [ [a[i][j] * b[i][j] : j in [1..m]]  : i in [1..n] ] );
	};
	
	getNabla_b_and_w := procedure() {
		m_nabla_b := [ la_matrix( [[0]*#b[1]] * #b ) : b in this.biases ];
		m_nabla_w := [ la_matrix( [[0]*#w[1]] * #w ) : w in this.weights ];
		
		return [m_nabla_b, m_nabla_w];
	};

	// Returns sum of correct guesses after feedforwarding
	evaluate := procedure(test_data) {
		test_results := [];
		
		// x: calculated number; y: real number
		for( [x,y] in test_data ) {
			// Calculate index of max. output
			out := feedforward(x);
			max := out[1][1];
			index := 1;
			for(i in {2..#out}) {
				if( out[i][1] > max ) {
					max := out[i][1];
					index := i;
				}
			}
			// Index-1 is output number
			test_results += [ [index-1,y] ];
		}
		
		// Return sum of correct guesses
		return #[1 : [x,y] in test_results | x == y];
	};

	// Return vector of partial derivates
	cost_derivative := procedure(output_activations, y) {
		return (output_activations - y);
	};

	// Sigmoid function for matrices
	// 1.0/(1.0+np.exp(-z))
	sigmoid_matrix := procedure(z) {
		// z is a matrix, so the function has to be used on every part of it
		return la_matrix([ [ 1.0/(1.0 + exp(- z[i][j] )) : j in [1..#z[1]] ] : i in [1..#z] ]);
	};
	
	sigmoid_vector := procedure(z) {
		return la_vector([ 1.0/(1.0 + exp(- z[i] )) : i in [1..#z] ]);
	};

	// Derivative of the sigmoid function
	// sigmoid(z)*(1-sigmoid(z))
	sigmoid_prime := procedure(z) {
		z := la_matrix( [ [z[i]] : i in [1..#z] ] );
		s := sigmoid_matrix(z);

		return la_matrix([ [ s[i][j] * (1 - s[i][j]) : j in [1..#s[1]] ] : i in [1..#s] ]);
	};

	// biases: [[b_layer1_neu1, b_layer1_neu2, ...], [b_layer2_neu1, ...], ...]
	computeRndBiases := procedure() {
		// Compute random biases in a shape fitting the network
		this.biases := [ 
			la_matrix([
				[((random()-0.5)*2)/28] : j in [1..sizes[i]]
			]) : i in [2..num_layers] 
		];
	};

	// weights: [ [[w1_layer1_neu1, w1_layer1_neu2, ...], [w2_layer1_neu1, ...], ...], [[w1_layer2_neu1, w1_layer2_neu2, ...], [w2_layer2_neu1, ...], ...] ]
	computeRndWeights := procedure() {
		// Compute random weights in a shape fitting the network
		this.weights := [ 
			la_matrix([
				[
					((random()-0.5)*2)/28 : k in [1..sizes[i]] 		// k'st-output of each neuron
				] : j in [1..sizes[i+1]] 							// each output of one layer
			]) : i in [1..num_layers-1]								// each layer (except output layer)
		];
	};
}