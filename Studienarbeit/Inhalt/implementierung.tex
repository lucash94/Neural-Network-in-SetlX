\chapter{Implementierung}

\section{Laden und Aufbereitung der MNIST Daten}
Für das Neuronale Netzwerk zur Erkennung von hangeschriebenen Zeichen zwischen werden Test- und Trainingsdaten der MNIST Datenbank genutzt. Die Datensätze können unter folgender Adresse gefunden werden: 
\\[0.2cm]
\hspace*{1.3cm}
\href{http://yann.lecun.com/exdb/mnist/}{http://yann.lecun.com/exdb/mnist/}
\\[0.2cm]
Da der MNIST Datensatz lediglich in Form von Binärdateien vorliegt und es in der aktuellen Version von SetlX nicht möglich ist Binärdateien zu lesen, wurde statt dem original Datensatz der umgewandelte Datensatz in Form einer CSV-Datei verwendet. Die Dateien können hier heruntergeladen werden:
\\[0.2cm]
\hspace*{1.3cm}
\href{https://pjreddie.com/projects/mnist-in-csv/}{https://pjreddie.com/projects/mnist-in-csv/}
\\[0.2cm]
Die Verwendung des CSV-Formats führt dazu, dass die Größe der Datensätze auf Grund fehlender Komprimierungen ansteigt. Ebenso wird das Einlesen der Datensätze langsamer, was an der eigentlichen Funktion des neuronalen Netzes allerdings nichts ändert und somit für dieses Projekt vertretbar ist. \\
Verwendet werden die CSV-Dateien $\mathtt{mnist\_test.csv}$ und $\mathtt{mnist\_train.csv}$. Die Traingsdaten umfassen insgesamt 60.000 Datensätze und die Testdaten 10.000 Datensätze. 
Die einzelnen Datensätze, also die handgeschriebenen Zeichen, sind in den Dateien in folgendem Format gespeichert:
\begin{center}
	$\mathtt{label1, pixel11, pixel12, pixel13, ...}$ \\
	$\mathtt{label2, pixel21, pixel22, pixel23, ...}$ \\
	...
\end{center}
Hierbei bezeichnet Label den Wert der jeweils gezeichneten Ziffer (wird benötigt um zu überprüfen, ob das Netzwerk die korrekte Ziffer identifiziert hat und um das Netzwerk zu trainieren). \\ \\
Um die Datensätze nun in SetlX importieren zu können, wird die Datei $\mathtt{csv\_loader.stlx}$ verwendet. Wird die Datei im SetlX-Interpreter ausgeführt, so liest sie die CSV-Dateien der Test- und Trainingsdaten (die Dateien müssen im selben Verzeichnis liegen und den oben erwähnten Namen haben) und speichert die Daten in den Variablen $\mathtt{test\_data}$ sowie $\mathtt{training\_data}$. Die Testdaten sind hierbei als Liste von Paaren in Form folgender Form abgelegt: \\
$[$ \\ 
$[\mathtt{pixels1},\ \mathtt{label1}],$ \\ 
$[\mathtt{pixels2},\ \mathtt{label2}],$ \\
$...]$ \\
Die Trainingsdaten sind prinzipiell nach dem gleichen Prinzip aufgebaut, allerdings wird hier für spätere Auswertungszwecke der Wert der Ziffer nicht als konkrete Zahl gespeichert, sondern in vektorisierter Form. Der vektorisierte Wert einer Zahl wird hier durch einen Vektor dargestellt, dessen Inhalt immer 0 ist, außer an der $\mathtt{label+1}$-ten Stelle. Dies entspricht dann genau der Form der Ausgabe des Netzwerkes. Beispielhaft würde eine Ziffer mit dem Wert $7$ als folgender Vektor dargestellt werden: \\
$<<0\ 0\ 0\ 0\ 0\ 0\ 0\ 1\ 0\ 0>>$ \\

\noindent
Auf eine genaue Beschreibung der Implementierung des Ladevorgangs wird in dieser Studienarbeit verzichtet, da hierbei keine komplexen Funktionen angewandt wurden und das Verfahren nicht relevant für das Verständnis neuronaler Netze an sich ist.

\section{Implementierung des neuronalen Netzes}
Dieser Abschnitt beschreibt die eigentliche Implementierung des neuronalen Netzwerkes zur Erkennung von handgeschriebenen Ziffern in SetlX. Um den Code möglichst kompakt zu halten, wurden die in den Originaldateien enthaltenen Kommentarzeilen in dieser Seminararbeit zum größten Teil entfernt.
Bei der Umsetzung des Netzwerkes in SetlX wird der SGD-Algorithmus als Lernmethode des Netzwerkes benutzt. Die im vorherigen Kapitel importierten Daten des MNIST-Datensatzes dienen als Grundlage der Ziffernerkennung. 
Das Netzwerk wird als Klasse in SetlX angelegt und enthält die folgenden Membervariablen:

\begin{enumerate}
\item $\mathtt{mNumLayers}$: Anzahl der Layer des aufzubauenden Netzwerkes 
\item $\mathtt{mSizes}$: Aufbau des Layers in Listenform. Bsp.: $[784, 30, 10]$ beschreibt ein Netzwerk mit 784 Inputfeldern, 30 Neuronen im zweiten (hidden) Layer und 10 Output-Neuronen
\item $\mathtt{mBiases}$: Alle Vorbelastungen des Netzwerkes (genauer Aufbau wird im Folgenden erläutert)
\item $\mathtt{mWeights}$: Alle Gewichte des Netzwerkes (genauer Aufbau wird im Folgenden erläutert)
\end{enumerate}

\noindent
Die Initialisierung des Netzwerkes zur Ziffernerkennung erfolgt durch folgende Befehle:
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    net := network([784, 30, 10]);
    net.init();
\end{Verbatim}
Als Übergabeparameter bei der Erstellung eines Netzwerk-Objektes wird die Struktur des Netzwerkes in Form einer Liste übergeben. Diese wird dann lediglich $\mathtt{mSizes}$ zugeordnet und basierend hierrauf wird $\mathtt{mNumLayers}$ ermittelt.
Die $\mathtt{init()}$-Funktion der $\mathtt{network}$-Klasse wird verwendet um die Gewichte und Vorbelastungen des Netzwerkes initial zufällig zu belegen. Hiermit werden Ausgangswerte gesetzt, welche später durch das Lernen des Netzwerkes angepasst werden.
Im Folgenden sind die verwendeten Funktionen, welche während der Gewichts- und Vorbelastungs-Initialisierung verwendet werden, zu sehen.
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    init := procedure() {
        computeRndBiases();
        computeRndWeights();
    };
    computeRndBiases := procedure() {
        this.mBiases := [ 
            computeRndMatrix([1, mSizes[i]]) : i in [2..mNumLayers] 
        ];
    };
    computeRndWeights := procedure() {
        this.mWeights := [ 
            computeRndMatrix([mSizes[i], mSizes[i+1]]) : i in [1..mNumLayers-1] 
        ];
    };
    computeRndMatrix := procedure(s) {
        [i,j] := s;
        return la_matrix([
            [ ((random()-0.5)*2)/28 : p in [1..i] ] : q in [1..j]
        ]);
    };
\end{Verbatim}

\begin{enumerate}
\item $\mathtt{init()}$: in der init-Funktion werden die Funktionen computeRndBiases() und $\mathtt{computeRndWeights()}$ aufgerufen
\item $\mathtt{computeRndBiases()}$: Die Funktion befüllt die Variable mBiases mit zufälligen Werten. Der für das Netzwerk benötigte Aufbau der Vorbelastungen entspricht folgender Form: \\
$[ \\ 
<<\ <<\mathtt{b\_layer1\_neu1}>>\ <<\mathtt{b\_layer1\_neu2}>>\ ...\ >>, \\
<<\ <<\mathtt{b\_layer2\_neu1}>>\ ...\ >>, \\
 ...\ ]$ \\
Das heißt es kann auf die Vorbelastungen mit folgendem Schema in SetlX zugegriffen werden: \\
\begin{center}
	$\mathtt{mBiases[layer][neuron][bias]}$
\end{center}
Hierbei ist zu beachten, dass der Wert für $\mathtt{bias}$ immer 1 ist, da jedes Neuron nur eine einzige Vorbelastung besitzt. Da es sich bei der Eingabe-Schicht des Netzwerkes nicht um Sigmoid-Neuronen handelt, sondern lediglich um Eingabewerte, werden hierfür keine Vorbelastungen benötigt. Deshalb wird bei der Erstellung der zufälligen Vorbelastungen nur $[2..\mathtt{mNumLayers}]$ (also alle Schichten außer dem Ersten) betrachtet.
\item $\mathtt{computeRndWeights()}$: Diese Funktion ist equivalent zu der Vorbelastungs-Funktion, lediglich wird folgende Struktur der Gewichte angelegt: \\
$[ \\ 
<<\ <<\mathtt{w1\_layer2\_neu1}\ \mathtt{w1\_layer2\_neu2}\ ...\ >>\ << \mathtt{w2\_layer2\_neu1}\ ...\ >>\ ...\ >>, \\
<<\ <<\mathtt{w1\_layer3\_neu1}\  \mathtt{w1\_layer3\_neu2}\ ...\ >>\ <<\mathtt{w2\_layer3\_neu1}\ ...\ >>\ ...\ >>, \\
...\ ]$ \\
Dies entspricht folgenden Zugriffsmöglichkeiten: \\ 
\begin{center}
	$\mathtt{mWeights[layer-1][neuron][weight\ for\ input\ neuron]}$
\end{center}
Der Zugriff auf die Schichten mittels $\mathtt{[layer-1]}$ resultiert aus den fehlenden Gewichten der Eingabe-Schicht.
\item $\mathtt{computeRndMatrix()}$: Diese Hilfsfunktion dient zur Erstellung der Struktur der Gewichte und Vorbelastungen in den zuvor vorgestellten Funktionen. Die Funktion enthält als Parameter eine Matrix-Struktur in Listenform und liefert die zugehörige Matrix mit zufälligen Werten zwischen $-1/28$ und $1/28$ zurück. Der Wert 28 ergibt sich aus der Größe des Eingabevektors (28x28 Pixel).  Die übergebende Struktur hat die Form $\mathtt{[x,y]}$, wobei $\mathtt{x}$ die Anzahl der Spalten und $\mathtt{y}$ die Anzahl der Reihen angibt. \\
Bsp.: $s := [1,2]\ \rightarrow\ <<\ <<x>>\ <<y>>\ >>$ und $s := [2,1]\ \rightarrow\ <<\ <<x\ y>>\ >>$
\end{enumerate}

\noindent
Sei nun $W$ die Matrix der Gewichte und $B$ die Matrix aller Vorbelastungen und $\vec{a}$ bezeichnet den Aktivierungsvektor der vorherigen Schicht, also deren Ausgabe (zu Beginn also die Pixel der Eingabe). Nach Gleichung xyz zur Berechnung einer Sigmoid-Ausgabe lässt sich nun folgende Formel aufstellen: 
\begin{equation}\label{eq:feedforwarding}
	\vec{a}' = \sigma(W*\vec{a} + B)
\end{equation}
\noindent
Hierbei bezeichnet $\vec{a}'$ den Ausgabe-Vektor der aktuellen Schicht, welcher dann der nächsten Schicht weitergeleitet wird (feedforwarding). Nachfolgend sind die Implementierungen der Sigmoid-Funktionen sowie dem Feedforwarding zu sehen.
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    feedforward := procedure(a) {
        a := la_vector(a);	
        for( i in {1..#mBiases} ) { 
            a := sigmoid( (mWeights[i]*a) + mBiases[i] );
        }
        return a;
    };                            
    sigmoid := procedure(z) {
        return la_vector([ 1.0/(1.0 + exp(- z[i] )) : i in [1..#z] ]);
    };
    sigmoid_prime := procedure(z) {
        s := sigmoid(z); 
        return la_matrix([ [ s[i] * (1 - s[i]) ] : i in [1..#s] ]);
    };
\end{Verbatim}
\begin{enumerate}
\item $\mathtt{feedforward(a)}$: Zunächst werden die als Liste übergebenen Eingabewerte $\mathtt{a}$ (784 Pixel in Listenform) mit Hilfe von $\mathtt{la\_vektor()}$ in einen Vektor umgewandelt und anschließend wird die Gleichung \eqref{eq:feedforwarding} auf alle Schichten des Netzwerkes angewandt. Zurückgegeben wird die resultierende Ausgabe jedes Neurons der letzten Schicht in vektorisierter Form.
\item $\mathtt{sigmoid(z)}$: Diese Funktionen nimmt einen Vektor $\mathtt{z}$ und berechnet mit Hilfe der Sigmoid-Formel (siehe Formel xyz) die Ausgabe der Neuronen in vektorisierter Form.
\item $\mathtt{sigmoid\_prime(z)}$: Für einen gegebenen Vektor $\mathtt{z}$ wird die Ableitung der Sigmoid-Funktion (nach Formel xyz) berechnet und in vektorisierter Form zurückgegeben.
\end{enumerate}
\noindent
Die Feedforward-Funktion dient also dazu, die Eingabewerte durch das gesamte Netzwerk durchzureichen und die daraus resultierende Ausgabe zu ermitteln. Als nächstes wird der Algorithmus diskutiert, durch welchem es dem Netzwerk ermöglicht wird zu \glqq lernen\grqq. Hierfür wird der SGD-Algorithmus verwendet. Die Implementierung des SGDs in SetlX ist nachfolgend aufgezeigt und wird nun im Detail erläutert.

\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    sgd := procedure(training_data, epochs, mini_batch_size, eta, test_data) {
        if(test_data != null) {
            n_test := #test_data; 		
        }
        n := #training_data;
		
        for(j in {1..epochs}) {
            training_data := shuffle(training_data);
            mini_batches := [ 
                training_data[k..k+mini_batch_size-1] : k in [1,mini_batch_size..n] 
            ];
			
            for(mini_batch in mini_batches) {
                update_mini_batch(mini_batch, eta);
            } 
			
            if(test_data != null) {
                ev := evaluate(test_data);
                print("Epoch $j$: $ev$ / $n_test$");
            }
            else {
                print("Epoch $j$ complete");
            }
        }
    };
\end{Verbatim}
\begin{enumerate}
\item Zeile 1: Übergabeparameter der Funktion sind die Trainingsdatensätze (Liste von Tupeln $\mathtt{[x,y]}$ mit $\mathtt{x}$ als Eingabewerten und $\mathtt{y}$ als gewünschtem Ergebnis), die Anzahl der Epochen (Integer-Wert), die Größe der Mini-Batches (Integer-Wert), die gewünschte Lernrate (Fließkomma-Wert) und den optionalen Testdatensätzen (äquivalenter Aufbau zu Trainingsdaten).
\item Zeile 7: Der nachfolgende Programmcode wird entsprechend der übergebenen Epochenanzahl mehrfach ausgeführt.
\item Zeile 8-12: Zuerst werden alle Trainingsdaten zufällig vermischt und anschließend Mini-Batches (also Ausschnitte aus dem Gesamtdatensatz) der vorher festgelegten Größe aus den Trainingsdaten extrahiert. Somit wird eine zufällige Belegung von Mini-Batches garantiert. Alle Mini-Batches werden in Listenform in der Variablen $\mathtt{mini\_batches}$ gespeichert.
\item Zeile 14-16: Anschließend wird für jeden Mini-Batch aus $\mathtt{mini\_batches}$ ein Schritt des SGD angewendet. Dies geschieht mit Hilfe der Funktion $\mathtt{update\_mini\_batches}$, welche anschließend genauer erläutert wird. Zweck der Funktion ist es die Gewichte und Vorbelastungen des Netzwerkes mit Hilfe einer Iteration des SGD-Algorithmus anzupassen. Die Basis für diese Anpassung liefert der übergebene Mini-Batch und die Lernrate.
\item Zeile 19-25: Dieser Programmcode dient zur Ausgabe auf der Konsole und teilt dem Benutzer die aktuelle Anzahl an korrekt ermittelten Datensätzen der Trainingsdaten nach jeder Epoche mit. Hierfür wird die Hilfsfunktion $\mathtt{evaluate}$ verwendet, welche unter Berücksichtigung des aktuellen Netzwerkzustandes die Outputs ermittelt, welcher bei Eingabe der Testdaten durch das Netzwerk errechnet wurden (genaue Implementierung folgt). Sollten der $\mathtt{sgd}$-Funktion keine Testdaten übergeben worden sein, so entfällt diese Ausgabe.
\end{enumerate}

\noindent
Nun wird die Funktion $\mathtt{evaluate}$ diskutiert, welche in der $\mathtt{sgd}$-Funktion aufgerufen wurde und dazu dient die Anzahl der vom Netzwerk korrekt ermittelten Datensätze zu berechnen. Die Funktion ist durch folgenden Code gegeben:
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    evaluate := procedure(test_data) {
        test_results := [0 : i in [1..#test_data]];
		
        i := 1;
        for( [x,y] in test_data ) {
            out := feedforward(x);
            max := out[1];
            index := 1;
            for(i in {2..#out}) {
                if( out[i] > max ) {
                    max := out[i];
                    index := i;
                }
            }
            test_results[i] := [index-1,y];
            i += 1;
        }
		
        return #[1 : [x,y] in test_results | x == y];
    };
\end{Verbatim}
\begin{enumerate}
\item Zeile 1: Der Funktion werden Datensätze in Listenform mitgegeben. Die Datensätze bestehen aus Tupeln der Form $[x,y]$, wobei $x$ die Pixel des jeweiligen Zeichens darstellt und $y$ der Wert des Zeichens ist.
\item Zeile 2: $\mathtt{test\_results}$ speichert die vom Netzwerk ermittelte Ausgabe, sowie die tatsächliche Ausgabe in Tupelform für jeden Datensatz. Die Variable wird zunächst mit 0-en initialisiert. Es könnte ebenso eine leere Liste erstellt werden, welche im späteren Verlauf um weitere Elemente erweitert wird, allerdings würde das Anhängen an die Liste zu erhöhtem Rechenaufwand führen was die Leistung des Netzwerkes negativ beeinflussen würde.
\item Zeile 5: Der nachfolgende Programmcode wird nun auf jedes Tupel $[x,y]$ des übergebenen Testdatensatzes angewandt.
\item Zeile 6-14: Mit Hilfe der bereits besprochenen Feedforward-Funktion wird zunächst die vektorisierte Ausgabe des Netzwerkes für den jeweiligen Datensatz berechnet und in $\mathtt{out}$ gespeichert. anschließend wird über den Ausgabe-Vektor iteriert und das Maximum sowie der dazugehörige Index im Vektor ermittelt \footnote{Anmerkung: Das Maximum könnte auch ohne eine Schleife mit Hilfe der $\mathtt{max}$-Funktion von SetlX ermittelt werden. Allerdings ist es so nicht möglich den zugehörigen Index zu berechnen}. 
\item Zeile 15: Die ermittelte Ziffer ergibt sich nun aus dem Index subtrahiert mit 1, da die Ziffern mit 0 beginnend im Ausgabevektor gespeichert sind. In die Variable $\mathtt{test\_results}$ wird nun der errechnete Wert sowie der tatsächliche Wert ($y$) gespeichert.
\item Zeile 19: Die Funktion gibt im Anschluss die Anzahl aller übereinstimmenden Ergebnisse in $\mathtt{test\_results}$ zurück.
\end{enumerate}

\noindent
ToDo: update-mini-batch, backprop \\

\noindent
Eine vorgefertigte Prozedur zur Initialisierung des benötigten Netzwerkes mit Beispielparametern befindet sich in der Datei $\mathtt{start.stlx}$, welche mit dem Befehl $\mathtt{setlx\ start.stlx}$ über die Konsole gestartet werden kann.

\section{Animation}