\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Percetron mit den Eingaben $x_1, x_2, x_3$ und der Ausgabe $output$.}}{2}{figure.2.1}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Unterschiedliche M\IeC {\"o}glichkeiten der Entscheidungsfindung.}}{3}{figure.2.2}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Percetron mit zwei Eingaben -2 und einem Bias von 3.}}{3}{figure.2.3}
\contentsline {figure}{\numberline {2.4}{\ignorespaces $\mathtt {NAND}$ Gatter mit den Eingaben $x_1$ und $x_2$.}}{4}{figure.2.4}
\contentsline {figure}{\numberline {2.5}{\ignorespaces $\mathtt {NAND}$ Gatter Aufbau mit Perceptrons.}}{4}{figure.2.5}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Vereinfachter $\mathtt {NAND}$ Gatter Aufbau mit Perceptrons.}}{4}{figure.2.6}
\contentsline {figure}{\numberline {2.7}{\ignorespaces Modifizieren von Weights und Biases schaffen lernendes Netzwerk.}}{5}{figure.2.7}
\contentsline {figure}{\numberline {2.8}{\ignorespaces Sigmoid Funktion $\sigma (z)$.}}{6}{figure.2.8}
\contentsline {figure}{\numberline {2.9}{\ignorespaces Sigmoid Funktion $\sigma (z)$.}}{6}{figure.2.9}
\contentsline {figure}{\numberline {2.10}{\ignorespaces Architektur eines neuronalen Netzwerks.}}{7}{figure.2.10}
\contentsline {figure}{\numberline {2.11}{\ignorespaces Aufbau des neuronalen Netzwerks hinsichtlich der einzelnen Layer.}}{7}{figure.2.11}
\addvspace {10\p@ }
