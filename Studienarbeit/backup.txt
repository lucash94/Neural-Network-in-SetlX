\noindent
Im vorherigen Abschnitt wurden Perceptrons als ein Methode für die Entscheidungsfindung beschrieben. Ein weiterer Anwendungsfall besteht in der Berechnung von logischen Funktion wie z.B. $\mathtt{AND}$, $\mathtt{OR}$ und $\mathtt{NAND}$. Fällt die Betrachtung auf ein Perceptron mit 2 Eingaben deren Gewichtung jeweils den Wert -2 aufweisen und einen Bias von 3, so ergibt sich folgende Abbildung (siehe Abb. \ref{fig:perceptron_example}).
\begin{figure}[hbt]
	\centering
	\includegraphics[scale=0.6]{Bilder/perceptron_example}
	\caption{Percetron mit zwei Eingaben -2 und einem Bias von 3.} 
	\label{fig:perceptron_example} 
\end{figure}

\noindent
Weisen die Eingaben $x_1, x_2$ den Wert 0 auf, ergibt sich für den $\mathtt{output}$ den Wert 1. Sind die Eingabewerte für $x_1, x_2$ -1 ergibt sich für den $\mathtt{output}$ den Wert -1. Der Aufbau beschreibt somit einen $\mathtt{NAND}$ Gatter.\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcll}
	w \cdot x + b & = & \mathtt{output} \\[0.2cm]
	(-2)*0+(-2)*0+3 & = & 1 \\[0.2cm]
	(-2)*1+(-2)*1+3 & = & -1
\end{array}
$
\\[0.2cm]
$\mathtt{NAND}$ Gatter können verwendet werden, um die unterschiedlichsten Berechnungen durchzuführen. Im Folgenden fällt die Betrachtung auf die Addition von zwei Bits $x_1$ und $x_2$. Für die Berechnung wird die bitweise Summe $x_1 \oplus x_2$ gebildet, wobei ein $\mathtt{carry bit}$ den Wert 1 annimmt, sobald $x_1$ und $x_2$ gleich 1.
\begin{figure}[hbt]
	\centering
	\includegraphics[scale=0.6]{Bilder/nand_gatter}
	\caption{$\mathtt{NAND}$ Gatter mit den Eingaben $x_1$ und $x_2$.} 
	\label{fig:nand_gatter} 
\end{figure}

\noindent
Um ein gleichwertiges Netzwerk abzuleiten, werden die $\mathtt{NAND}$ Gatter durch Perceptrons mit jeweils 2 Eingaben ersetzt. Hierbei weisen die Gewichtungen $w_1, w_2$ den Wert -2 und der Bias $b$ den Wert 3 auf. 
\begin{figure}[hbt]
	\centering
	\includegraphics[scale=0.6]{Bilder/nand_gatter_perceptron}
	\caption{$\mathtt{NAND}$ Gatter Aufbau mit Perceptrons.} 
	\label{fig:nand_gatter_perceptron} 
\end{figure}

\noindent
In einem weiteren Schritt wird die Abbildung eines $\mathtt{NAND}$ Gatter mit Perceptrons vereinfacht. Dazu werden mehrere Eingänge eines Perceptrons zu einem zusammengefasst, weshalb aus den zwei Eingaben -2 der Wert -4 resultiert. Ebenfalls werden die Eingaben in einem sogenannten \textit{Input Layer} zusammengefasst, wobei durch die Notation eine Eingabe nicht mit einem Perceptron gleichzustellen ist.
\begin{figure}[hbt]
	\centering
	\includegraphics[scale=0.6]{Bilder/nand_gatter_perceptron_simplified}
	\caption{Vereinfachter $\mathtt{NAND}$ Gatter Aufbau mit Perceptrons.} 
	\label{fig:nand_gatter_perceptron_simplified} 
\end{figure}

\noindent
Dieser Anwendungsfall zeigt, dass mit Perceptrons unterschiedliche Berechnungen durchgeführt werden können. Implementierte Lernalgorithmen können Gewichtungen sowie den Bias automatisch durch entsprechende Stimuli im Netzwerk anpassen und ermöglichen die Nutzung von künstliche Neuronen, die sich von herkömmlichen Logik Gattern unterscheiden. Neuronale Netze können somit über einen definierten Zeitraum lernen, wie bestimmte Probleme zu lösen sind. 